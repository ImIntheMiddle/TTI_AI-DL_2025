{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgS6Ia4MaUMI"
      },
      "source": [
        "# 人工知能・深層学習実験 第６・７回\n",
        "\n",
        "コンピュータビジョンでは，人間が視覚を通して周囲の世界を認識するように，コンピュータによって自動的に画像や動画の持つ意味を認識することを目指します．  \n",
        "\n",
        "今回の実験では，コンピュータビジョン分野の中で「人間には簡単だが機械には難しい」問題の例として半ばジョーク的に挙げられることの多い唐揚げ・プードル分類に挑戦しましょう．\n",
        "\n",
        "\n",
        "参考：唐揚げとプードル以外にも，マフィンとチワワなどが有名です．  \n",
        "https://front-row.jp/_ct/17135189\n",
        "\n",
        "参考：実装にあたり，以下の記事を参考にしました．\n",
        "https://qiita.com/katsujitakeda/items/f1842f5e831bb6475ba8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r39D9jHQf4m2"
      },
      "source": [
        "# 実験概要\n",
        "実験は以下の流れで進めます．\n",
        "\n",
        "1.   スクレイピングによる画像データの収集\n",
        "2.   データセットの構築とデータの前処理\n",
        "3.   畳み込みニューラルネットワークの構築\n",
        "4.   画像分類の学習と評価\n",
        "5.   1～4の改善による分類性能の向上\n",
        "6.   分類性能のテスト\n",
        "\n",
        "6のテストはコンペ形式で，各自のモデルの分類性能を競ってもらいます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URPy8nt7Gocf"
      },
      "source": [
        "# 0. 下準備\n",
        "google colabで動作確認しています．\n",
        "ローカル環境でうまく動かない場合は，colabに切り替えてみてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmlgUBJA51g7"
      },
      "source": [
        "## 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJeRSf3m5udA"
      },
      "outputs": [],
      "source": [
        "# シェルコマンドを使用\n",
        "!pip install icrawler\n",
        "!pip install japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AonEZb3t5y38"
      },
      "source": [
        "## 必要なライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe5w8C4Qd9IK"
      },
      "outputs": [],
      "source": [
        "# 基本的なライブラリ\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 画像のスクレイピング\n",
        "from icrawler.builtin import GoogleImageCrawler # Google経由で画像を拾ってくるライブラリ\n",
        "\n",
        "# 画像データの処理\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# CIFARデータセット\n",
        "from torchvision.datasets import CIFAR10 as CIFAR\n",
        "\n",
        "# ネットワークの構築，学習\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "# 可視化関連\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "from PIL import Image\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrqZYXhr5_R2"
      },
      "source": [
        "## シードの固定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clwuF2tM5H4c"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WPB3fVuGkbA"
      },
      "source": [
        "# 1. スクレイピングによる画像データの収集\n",
        "\n",
        "上でインポートした*GoogleImageCrawler*を使ってスクレイピングを実行し，指定したディレクトリに画像を保存します．\n",
        "\n",
        "参考（icrawlerのドキュメント）: https://icrawler.readthedocs.io/en/latest/builtin.html\n",
        "\n",
        "デフォルトでは，`'dataset'`というディレクトリ以下に各検索キーワードのディレクトリが作成され，その中に画像が保存されます (colab環境の場合，サイドバーの「ファイル」からファイル構成を確認できます)．\n",
        "\n",
        "スクレイピング実行時に画像を上手く読み込めずエラー文が表示されることがありますが，ある程度データが集まっていれば気にしなくて良いです．\n",
        "\n",
        "演習 1.1:   以下のコードを実行し，実行結果を確認してみましょう．  \n",
        "\n",
        "演習 1.2:   スクレイピングするキーワードや枚数を変更してみましょう．  \n",
        "\n",
        "演習 1.3:   スクレイピング結果から気づいたことを議論してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOt_bRwZeG2U"
      },
      "outputs": [],
      "source": [
        "# 画像のスクレイピング\n",
        "def scrape_imgs(keywords, max_num, save_dir='dataset'):\n",
        "    for keyword in keywords.keys():\n",
        "        print(f'Scraping {keyword} images...')\n",
        "        num_img = 0\n",
        "        if not os.path.exists(os.path.join(save_dir, keywords[keyword])):\n",
        "            os.makedirs(os.path.join(save_dir, keywords[keyword]))\n",
        "        crawler = GoogleImageCrawler(\n",
        "            downloader_threads = 4,\n",
        "            storage = {'root_dir' : os.path.join('tmp/', keywords[keyword])}\n",
        "        )\n",
        "        crawler.crawl(\n",
        "            keyword = keyword,\n",
        "            max_num = max_num,\n",
        "            filters = dict(date = ((2020,1,1),(2023,12,31)))\n",
        "        )\n",
        "        # 拡張子をpngに統一\n",
        "        for i, filename in enumerate(glob.glob(os.path.join('tmp', keywords[keyword], '*.jpg'))):\n",
        "            try:\n",
        "                img = Image.open(filename)\n",
        "                img.save(filename.replace('.jpg', '.png').replace('tmp', save_dir))\n",
        "                num_img += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Could not open or process {img_path}: {e}\")\n",
        "                continue # 開けない画像はスキップ\n",
        "        print(f'Saved {num_img} {keyword} images.')\n",
        "\n",
        "    print(f'Done! Saved to {save_dir}!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWTu6z44zi44"
      },
      "outputs": [],
      "source": [
        "# スクレイピングするキーワードを定義\n",
        "keywords = {'唐揚げ': 'karaage', 'トイプードル': 'poodle'} # 2クラス分類\n",
        "\n",
        "# スクレイピングを実行\n",
        "max_num = 30 # 各キーワードの画像を最大で何枚集めたいか（実際にはエラーで取得に失敗するものがある）\n",
        "!rm -rf dataset\n",
        "scrape_imgs(keywords, max_num)\n",
        "!rm -rf tmp # 一時保存先を消去"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHWw1Wvdt430"
      },
      "source": [
        "# 2. データセットの構築とデータの前処理\n",
        "\n",
        "*PyTorch*では*Dataset*クラスが定義されており，その*Datase*tクラスを継承した新たなクラスを定義することで独自のデータセットを構築することができます．\n",
        "\n",
        "ここでは，スクレイピングで収集した画像データを用いて画像分類のためのデータセットを構築してみましょう．\n",
        "\n",
        "演習 2.1:   下記のコードがエラーなく実行できることを確かめてください．  \n",
        "\n",
        "演習 2.2:   `len()`, `show_img()`などのメソッドを実行し，結果を確認してみましょう．\n",
        "\n",
        "演習 2.3:   配布資料では前処理として画像のリサイズと画素値の正規化を行っていますが，`transform_train`に様々な前処理を追加することでデータセットを水増し（*augmentation*）することもできます．`RandomRotation()`などのメソッドを試し，前処理によって画像の可視化結果がどうなるかを確認してください．\n",
        "\n",
        "参考（PyTorchドキュメント）: https://pytorch.org/vision/main/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFIjx29umFru"
      },
      "outputs": [],
      "source": [
        "class ScrapedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, class_names, img_size, transform, img_dir='dataset/'):\n",
        "        self.classes = class_names\n",
        "        self.class_name_list = list(self.classes.keys())\n",
        "        self.label_map = {v: k for k, v in enumerate(self.classes)} # 各クラス名にIDを紐づける\n",
        "        self.img_dir = img_dir\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "        self.img_paths, self.labels = self.make_dataset() # 各画像データの保存先とクラス名を取得する\n",
        "\n",
        "    def make_dataset(self):\n",
        "        \"\"\"\n",
        "        classes: dict\n",
        "        img_dir: path to dataset\n",
        "        \"\"\"\n",
        "        print('Making dataset...', end='')\n",
        "        img_paths = []\n",
        "        labels = []\n",
        "        for class_name, class_dir in self.classes.items():\n",
        "            class_imgs = glob.glob(os.path.join(self.img_dir, class_dir, '*.png')) # 画像のパスを読み出す\n",
        "            img_paths.extend(class_imgs)\n",
        "            labels.extend([self.label_map[class_name]] * len(class_imgs)) # クラス名に対応するIDを付与\n",
        "        print('Done!')\n",
        "        return img_paths, labels\n",
        "\n",
        "    def __len__(self): # データセットのサイズを定義するためのメソッド．必須．\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def read_img(self, idx):\n",
        "        # 画像の読み込み\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path, mode='r') # mode: RGB\n",
        "        return img # 読み込んだPIL Imageを返す\n",
        "\n",
        "    def __getitem__(self, idx): # データセットの要素を取得する際の処理を示すメソッド．必須．\n",
        "        img = self.read_img(idx) # 画像の読み込み\n",
        "        img_tensor = self.transform(img) # 画像の前処理\n",
        "        label_int = self.labels[idx] # 画像に対応する正解ラベルを取得\n",
        "        return img_tensor, label_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IruqdL8a8f2D"
      },
      "outputs": [],
      "source": [
        "# 画像サイズを定義\n",
        "img_size = (64, 64)\n",
        "\n",
        "# 学習用の前処理を定義 (必要に応じてデータ拡張を追加)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(), # 読み込んだPIL ImageをTensorに変換\n",
        "    transforms.Resize(img_size), # 指定した画像サイズにリサイズ\n",
        "    ### 新たな前処理を追加可能 ###\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 画素値の正規化．平均と分散の値はImageNetデータセットの値に準じる．\n",
        "])\n",
        "\n",
        "# 検証用・テスト用の前処理を定義\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(), # Tensorへの変換\n",
        "    transforms.Resize(img_size), # リサイズ\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 画素値の正規化\n",
        "])\n",
        "\n",
        "scraped_dataset = ScrapedDataset(class_names=keywords, img_size=img_size, transform=transform_train)\n",
        "\n",
        "### 穴埋め ###\n",
        "print(f'データセットのサイズ: {}')\n",
        "print(f'データセットのクラス名: {}')\n",
        "class_num_scraped = ### 穴埋め ###\n",
        "print('クラス数：', class_num_scraped)\n",
        "print(f'データセットのクラス名とラベルIDの対応関係: {}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTtbw7mNr1ru"
      },
      "outputs": [],
      "source": [
        "def show_img(sample, class_name_list, pred=None, normalize=True): # 正規化後の画像で可視化するか指定可能\n",
        "    img_tensor = sample[0]\n",
        "    label = class_name_list[sample[1]]\n",
        "    if pred is not None:\n",
        "        assert type(pred)==int\n",
        "        pred = class_name_list[pred]\n",
        "    plt.figure()\n",
        "    if pred:\n",
        "        plt.title(f'Label: {label} Pred: {pred}')\n",
        "    else:\n",
        "        plt.title(f'Label: {label}')\n",
        "    img_array = img_tensor.permute(1, 2, 0).numpy() # (C, H, W) -> (H, W, C)\n",
        "    if not normalize:\n",
        "        # 正規化を元に戻す\n",
        "        img_array = img_array * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    plt.imshow(img_array)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "### 引数を指定 ###\n",
        "show_img()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk7nzTd5WR8w"
      },
      "source": [
        "# 3. 畳み込みニューラルネットワークの構築\n",
        "コンピュータビジョン分野では，ニューラルネットワークの中でも**畳み込みニューラルネットワーク**（*CNN; Convolutional Neural Network*）と呼ばれるものが多く使われています． これは人間の視覚機能をヒントに発明されたもので，畳み込み（下記のコードでは`Conv2d()`）とプーリング（下記のコードでは`max_pool2d()`）と呼ばれる処理を繰り返すことで画像中の様々なパターンを捉えます．\n",
        "\n",
        "まずは，下記のシンプルなCNNを使って画像からクラスを分類してみましょう．\n",
        "\n",
        "演習 3.1: 下記のコードがエラーなく実行できることを確かめてください．  \n",
        "\n",
        "演習 3.2: 実行すると，データセット内の一つ目のデータに対するCNNの出力結果が表示されます．この出力は何を意味していますか？  \n",
        "\n",
        "演習 3.3: コードの中では，`Conv2d()`や`max_pool2d()`に`kernel_size`や`padding`といったオプションが設定されています．これらの意味や，他にどんなオプションがあるかを調べてみましょう．また，元コードではouthやoutw（各層を通った後の特徴量のサイズ）などの変数がハードコーディングされています．この値を自由に変更しても問題なく動作するように`init()`内の変数定義を書き換え，エラーなく実行が完了するようにしましょう．\n",
        "\n",
        "**ヒント：畳み込み層の前後で，入出力の次元の関係は以下の式を満たします（参考: https://qiita.com/DeepTama/items/379cac9a73c2aed7a082 ）．**\n",
        "\n",
        "**出力サイズ =｛(入力サイズ + 2 × パディングサイズ - カーネルサイズ) / ストライドサイズ｝+ 1**\n",
        "\n",
        "演習 3.4: 元のコードのCNNは，畳み込みとプーリングを2回ずつ行っています．ここにコードを書き加え，畳み込みとプーリングをもう1回ずつ実行するように変更してみてください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEK8LX_i7fyu"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, class_num, img_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.img_size = img_size  # 画像サイズ\n",
        "\n",
        "        # 畳み込み層1を定義（入力チャネルRGB=3、出力チャネル32、3x3カーネル）\n",
        "        self.conv1_outc = 32  # 特徴マップの数を増加\n",
        "        self.conv1 = nn.Conv2d(3, self.conv1_outc, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        # conv1後の出力サイズを計算\n",
        "        self.conv1_outh = 64\n",
        "        self.conv1_outw = 64\n",
        "\n",
        "        self.pool1_size = 2\n",
        "\n",
        "        # 畳み込み層2（出力チャネル64）\n",
        "        self.conv2_outc = 64\n",
        "        self.conv2 = nn.Conv2d(self.conv1_outc, self.conv2_outc, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        # conv2後の出力サイズ\n",
        "        self.conv2_outh = 32\n",
        "        self.conv2_outw = 32\n",
        "\n",
        "        self.pool2_size = 2\n",
        "\n",
        "        # 全結合層の入力サイズ（最後のプーリング出力を平坦化）\n",
        "        self.fc1_in = self.conv2_outc * 16 * 16\n",
        "        self.fc1_out = 32  # 中間層の出力ユニット数\n",
        "\n",
        "        # 全結合層の定義\n",
        "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
        "        self.fc2 = nn.Linear(self.fc1_out, class_num)  # 出力層はクラス数と同じ\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 畳み込み → Tanh → プーリング1\n",
        "        x = F.tanh(self.conv1(x))\n",
        "        assert x.shape[2] == self.conv1_outh and x.shape[3] == self.conv1_outw, 'テンソルのサイズ計算を見直してください．'\n",
        "        x = F.max_pool2d(x, kernel_size=self.pool1_size)\n",
        "\n",
        "        # 畳み込み → Tanh → プーリング2\n",
        "        x = F.tanh(self.conv2(x))\n",
        "        assert x.shape[2] == self.conv2_outh and x.shape[3] == self.conv2_outw, 'テンソルのサイズ計算を見直してください．'\n",
        "        x = F.max_pool2d(x, kernel_size=self.pool2_size)\n",
        "\n",
        "        # 特徴マップをベクトル化\n",
        "        x = x.view(batch_size, -1)\n",
        "        assert x.shape[1] == self.fc1_in\n",
        "\n",
        "        # 全結合層 → Tanh → 出力層\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrZcpNgs_MPG"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = CNN(class_num_scraped, img_size) # CNNを初期化\n",
        "model.to(device) # モデルをGPUまたはCPUに載せる\n",
        "\n",
        "# サンプルで動作を確認\n",
        "sample_input, label = scraped_dataset[0]\n",
        "sample_input = sample_input.unsqueeze(0)\n",
        "sample_input = sample_input.to(device)\n",
        "print(sample_input.shape)\n",
        "sample_output = model(sample_input)\n",
        "print(sample_output)\n",
        "\n",
        "print(scraped_dataset.class_name_list[sample_output.argmax()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9rm-ZYgCTK5"
      },
      "source": [
        "# 4. 画像分類の学習と評価\n",
        "ここまででデータセットとモデルを構築できたので，いよいよモデルの学習に移ります．\n",
        "\n",
        "今回の実験では2回目の最後にテストデータに対する分類性能を比較しますので，それまでは手元のデータを学習データと検証データに分割して使用します．\n",
        "\n",
        "演習 4.1: 以前の講義では自分自身で学習データと検証データに分割しましたが，PyTorchには`random_split()`のようにデータセットの分割を自動で行うメソッドが存在します．これを使って，学習データ，検証データ，テストデータを7:1:2の割合で分割してみましょう．\n",
        "\n",
        "参考（PyTorchのドキュメント）: https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
        "\n",
        "演習 4.2: 以前の講義資料を参考に，適切な損失関数と最適化手法を設定してみましょう．\n",
        "\n",
        "演習 4.3: 以前の講義資料を参考に，モデルの出力結果から特定のクラスを予測結果に定め，その結果を基に検証データにおける正解率を算出してみましょう．\n",
        "\n",
        "演習 4.4: 下記のコードがエラーなく実行できるようにしてください．\n",
        "\n",
        "演習 4.5: 検証データの中で，予測に成功した画像と失敗した画像をそれぞれ可視化してみましょう．何か傾向は見られるでしょうか．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPjyEFdClG5H"
      },
      "outputs": [],
      "source": [
        "# 学習用のループ\n",
        "def train(bar, model, train_loader, criterion, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    TP = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() # 勾配情報をリセット\n",
        "        outputs = model(images) # モデルの出力\n",
        "\n",
        "        loss = criterion(outputs, labels) # 出力と正解ラベルを比較し損失計算\n",
        "        loss.backward() # 誤差逆伝播\n",
        "        optimizer.step() # モデルのパラメータを更新\n",
        "\n",
        "        pred = ### 穴埋め ### # 予測クラスを取得する\n",
        "        TP += ### 穴埋め ### # ミニバッチ内のTPをカウントする\n",
        "\n",
        "        epoch_loss += copy.deepcopy(loss.item()) * images.size(0) # ミニバッチ内の損失を合計値に加算\n",
        "        samples += images.size(0)\n",
        "        accuracy = ### 穴埋め ### # 正解率を更新する\n",
        "        bar.set_description(f'Epoch {epoch+1} Train {samples/len(train_loader.dataset)*100:.0f}% Loss: {epoch_loss/samples:.4f}, Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    return epoch_loss / samples, accuracy\n",
        "\n",
        "# 検証用のループ\n",
        "def val(bar, model, val_loader, criterion, epoch, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    TP = 0\n",
        "    with torch.no_grad(): # モデルの重みを更新しないので勾配を計算しなくて良い\n",
        "        for i, (images, labels) in enumerate(val_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            pred = ### 穴埋め ### # 予測クラスを取得する\n",
        "            TP += ### 穴埋め ### # ミニバッチ内のTPをカウントする\n",
        "\n",
        "            epoch_loss += loss.item() * images.size(0) # エポック全体の損失の合計値を更新する\n",
        "            samples += images.size(0)\n",
        "            accuracy = ### 穴埋め ### # 正解率を更新する\n",
        "            bar.set_description(f'Epoch {epoch+1} Val {samples/len(val_loader.dataset)*100:.0f}% Loss: {epoch_loss/samples:.4f}, Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    return epoch_loss / samples, accuracy\n",
        "\n",
        "def loop(train_loader, val_loader, model, num_epoch, criterion, optimizer, device, save_name=\"model\"):\n",
        "    print('Training...', end='')\n",
        "    bar = tqdm.tqdm(range(num_epoch), leave=True)\n",
        "    result = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    for epoch in bar:\n",
        "        train_loss, train_acc = train(bar, model, train_loader, criterion, optimizer, epoch=epoch, device=device)\n",
        "        result['train_loss'].append(train_loss)\n",
        "        result['train_acc'].append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = val(bar, model, val_loader, criterion, epoch=epoch, device=device)\n",
        "        result['val_loss'].append(val_loss)\n",
        "        result['val_acc'].append(val_acc)\n",
        "    print(f'Done!')\n",
        "\n",
        "    # 学習したモデルの重み（パラメータ）を保存する\n",
        "    torch.save(model.state_dict(), os.path.join(save_name + \".pth\"))\n",
        "\n",
        "    # 学習中の性能の推移を可視化\n",
        "    report_result(result)\n",
        "\n",
        "def report_result(result):\n",
        "    # visualize the loss curve\n",
        "    plt.figure()\n",
        "    plt.plot(result['train_loss'], label='train')\n",
        "    plt.plot(result['val_loss'], label='val')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # visualize the accuracy curve\n",
        "    plt.figure()\n",
        "    plt.plot(result['train_acc'], label='train')\n",
        "    plt.plot(result['val_acc'], label='val')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HENPIMbTD6WV"
      },
      "outputs": [],
      "source": [
        "# 指定した条件で学習を実行\n",
        "dataset = \"CIFAR\"\n",
        "# dataset = \"SCRAPED\"\n",
        "\n",
        "# 画像サイズを定義\n",
        "img_size = (64, 64)\n",
        "\n",
        "# 学習用の前処理を定義 (必要に応じてデータ拡張を追加)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(), # 読み込んだPIL ImageをTensorに変換\n",
        "    transforms.Resize(img_size), # 指定した画像サイズにリサイズ\n",
        "    ### 新たな前処理を追加可能 ###\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 画素値の正規化．平均と分散の値はImageNetデータセットの値に準じる．\n",
        "])\n",
        "\n",
        "# 検証用・テスト用の前処理を定義\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(), # Tensorへの変換\n",
        "    transforms.Resize(img_size), # リサイズ\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 画素値の正規化\n",
        "])\n",
        "\n",
        "\n",
        "if dataset==\"SCRAPED\":\n",
        "    # 学習データ，検証データ，テストデータを分割\n",
        "    scraped_dataset = ScrapedDataset(class_names=keywords, img_size=img_size, transform=transform_train)\n",
        "    dataset_train, dataset_val, dataset_test = ### 穴埋め ###\n",
        "    class_num = class_num_scraped\n",
        "elif dataset==\"CIFAR\":\n",
        "    # CIFARの学習データセットを読み込み\n",
        "    cifar_dataset = CIFAR(root='./data', train=True, download=True, transform=transform_train)\n",
        "    show_img() ### 穴埋め ###\n",
        "\n",
        "    # 学習データと検証データを分割\n",
        "    dataset_train, dataset_val, _ = ### 穴埋め ###\n",
        "\n",
        "    # CIFARのテストデータセットを読み込み\n",
        "    dataset_test = CIFAR(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    # CIFARのクラス数に更新\n",
        "    class_num = len(cifar_dataset.classes)\n",
        "else:\n",
        "    raise Exception(\"データセットはCIFARかSCRAPEDから選択してください．\")\n",
        "\n",
        "print(f'学習データ数：{}，検証データ数：{}，テストデータ数：{}')\n",
        "print(f'クラス数: {class_num}')\n",
        "print(f'画像サイズ：{img_size}')\n",
        "\n",
        "# データローダーを定義\n",
        "train_loader = DataLoader(dataset_train, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(dataset_val, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(dataset_test, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# モデルを定義\n",
        "model = CNN(class_num, img_size) # CNNを初期化\n",
        "model.to(device) # モデルをGPUまたはCPUに載せる\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = ### 穴埋め ###\n",
        "\n",
        "# 最適化手法や学習率を定義\n",
        "optimizer = ### 穴埋め ###\n",
        "\n",
        "# 学習のエポック数を設定（デバッグ時は小さい値にすると良い）\n",
        "num_epoch = 5\n",
        "\n",
        "loop(train_loader, val_loader, model, num_epoch, criterion, optimizer, device, save_name=\"model_scraped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAKmje9gHFDd"
      },
      "source": [
        "# 5. 分類性能の向上\n",
        "ここまでで，データの収集，前処理，モデルの構築，モデルの学習を経て分類性能を評価することができました．最終的に得られるモデルの分類性能は，これらすべてに左右されて決まります．ここからは，これまでに実装してきた内容に独自に工夫を加え，分類性能の向上を目指してください．\n",
        "\n",
        "演習 5.1: 収集したデータの中に，学習に悪影響を与えそうなものはありませんか．そのような画像をデータセットから除いたり，収集するデータを増やしたりするとどうなりますか．\n",
        "\n",
        "演習 5.2: 前処理を変更することで性能がどのように変化するか確認しましょう．前処理にも適切なものとそうでないものがあります．\n",
        "\n",
        "演習 5.3: モデルの構成を見直し，性能を向上させてみましょう．層を増やすだけでなく，活性化関数などの種類も見直しましょう．\n",
        "\n",
        "演習 5.4: 学習条件を見直し，性能を向上させてみましょう．最適化手法や学習のエポック数など，調整できる点が多々あります．\n",
        "\n",
        "演習 5.5: 唐揚げとプードルの分類でtest accuracy 1.0 を達成しましょう．達成出来たらノートブック上部の`dataset`を`CIFAR`に切り替え，そちらの性能向上を目指してください．\n",
        "\n",
        "発展 5.6: モデルの予測結果に何か傾向がないか調べてみましょう．予測に成功/失敗した画像を可視化したり，クラスごとの正解率などの統計量を算出したりといった手があります．\n",
        "\n",
        "発展 5.7: 深層学習では，過学習と呼ばれる問題がよく発生します．過学習されたモデルはトレーニングデータに対して過剰にフィットし（答えの丸暗記のようなもの），トレーニングデータに対する性能は向上しますが検証データやテストデータに対する性能が悪化します．過学習の対策方法について調べてみましょう．学習データのバリエーションを増やすことはその一つです．\n",
        "\n",
        "発展 5.8: CNNに限らずよく用いられる強力なテクニックに残差接続（*residual connection*）というものがあるので，興味がある人は調べて実装してみると良いでしょう．\n",
        "\n",
        "参考（残差接続の解説と実装）: https://euske.github.io/introdl/lec8/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foKc0YiocysU"
      },
      "source": [
        "# 6. 分類性能のテスト\n",
        "下記のコードでは，学習時に`save_name.pth`に保存したモデルを用いてテストデータの画像クラスを予測します．\n",
        "唐揚げプードル分類とCIFAR10の分類それぞれで性能向上を目指してください．\n",
        "２回目の最後には，コンペ形式でCIFAR10での分類性能を評価します．\n",
        "正解率の向上以外にも，学習効率の改善など，独自の取り組みを期待します．\n",
        "\n",
        "取り組みのねらい，内容，結果をレポートにまとめてpdf形式で提出してください．\n",
        "\n",
        "コンペの注意点：\n",
        "- スクレイピングのデータ数は自由に増やして構いません．\n",
        "- スクレイピングのキーワードやクラス数も自由に変更して構いません．\n",
        "- モデルはCNN以外のものを独自に定義，使用しても構いません．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fcedda9"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            pred = ### 穴埋め ###\n",
        "            total += labels.size(0)\n",
        "            correct += ### 穴埋め ###\n",
        "\n",
        "    accuracy = ### 穴埋め ###\n",
        "    print(f'Test Accuracy: {accuracy*100:.2f} %')\n",
        "\n",
        "# モデルを再構築し、学習済みの重みをロード\n",
        "test_model = CNN(class_num, img_size)\n",
        "model_save_name = \"model_scraped.pth\" # 学習させたモデルファイル名に修正してください\n",
        "test_model.load_state_dict(torch.load(model_save_name, map_location=device))\n",
        "test_model.to(device)\n",
        "\n",
        "# テストデータで評価\n",
        "print(f'{dataset}データセットでの評価結果：')\n",
        "test(test_model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
